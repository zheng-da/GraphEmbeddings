{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import time\n",
    "import torch as th\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from load_data import load_ogb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code here shows the baseline of using MLP to classify nodes with the spectral embeddings of the nodes. Even though the spectral embeddings contain the global information of the graph. It seems a simple linear decomposition cannot achieve very good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(th.utils.data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, list_IDs, labels,features):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.features=features\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        X =self.features[ID]\n",
    "        y = self.labels[ID]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "class ClassifierMLP(th.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,out_size,single_layer=False):\n",
    "        super(ClassifierMLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.out_size=out_size\n",
    "        self.single_layer=single_layer\n",
    "        if self.single_layer:\n",
    "            self.fc1 = th.nn.Linear(self.input_size, self.out_size)\n",
    "        else:\n",
    "            self.fc1 = th.nn.Linear(self.input_size, self.hidden_size)\n",
    "            self.relu = th.nn.ReLU()\n",
    "            self.fc2 = th.nn.Linear(self.hidden_size, self.out_size)\n",
    "        self.activation=partial(F.softmax, dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.single_layer:\n",
    "            output=self.fc1(x)\n",
    "        else:\n",
    "            hidden = self.fc1(x)\n",
    "            relu = self.relu(hidden)\n",
    "            output = self.fc2(relu)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def compute_acc(results, labels, multilabel):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of prediction given the labels.\n",
    "    \"\"\"\n",
    "    if multilabel:\n",
    "        predicted_labels = th.sigmoid(results).data > 0.5\n",
    "        return th.sum(predicted_labels.cpu() == labels.cpu()).item() / labels.numel()\n",
    "    else:\n",
    "        labels = labels.long()\n",
    "        return (results == labels).float().sum() / len(results)\n",
    "\n",
    "def evaluate_mlp_model(model,data_loader,multilabel,device):\n",
    "    labels=None\n",
    "    for local_batch, local_labels in data_loader:\n",
    "        pred = model(local_batch)\n",
    "        local_labels = local_labels.to(device)\n",
    "\n",
    "        if multilabel is False:\n",
    "            pred = pred.argmax(dim=1)\n",
    "        else:\n",
    "            pred = pred\n",
    "        if labels is None:\n",
    "            labels=local_labels\n",
    "            logits=pred\n",
    "        else:\n",
    "            labels=th.cat((labels,local_labels))\n",
    "            logits = th.cat((logits, pred))\n",
    "\n",
    "    acc=compute_acc(results=logits, labels=labels, multilabel=multilabel)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def compute_acc_MLP_pytorch(feats, labels, train_idx, val_idx, test_idx, num_classes, device,\n",
    "                            n_hidden=100, lr_d=1e-4, n_cepochs=50,\n",
    "                            l2norm = 0.0001, batch_size=1000, multilabel=False):\n",
    "\n",
    "    ###\n",
    "    # Use the encoded features for classification\n",
    "    # Here we initialize the features using the reconstructed ones\n",
    "\n",
    "    # feats = g.nodes[category].data['features']\n",
    "\n",
    "    inp_dim = feats.shape[1]\n",
    "    model = ClassifierMLP(input_size=inp_dim, hidden_size=n_hidden,out_size=num_classes)\n",
    "    model = model.to(device)\n",
    "    feats=feats[np.arange(labels.shape[0])].to(device)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = th.optim.Adam(model.parameters(), lr=lr_d, weight_decay=l2norm)\n",
    "\n",
    "    # training loop\n",
    "    print(\"start training...\")\n",
    "    forward_time = []\n",
    "    backward_time = []\n",
    "    model.train()\n",
    "\n",
    "    # TODO find all zero indices rows and remove.\n",
    "    if len(labels.shape)>1:\n",
    "        zero_rows=np.where(~(labels).cpu().numpy().any(axis=1))[0]\n",
    "        print(\"Len of zero rows \"+str(len(zero_rows)))\n",
    "\n",
    "        train_idx=np.array(list(set(train_idx).difference(set(zero_rows))))\n",
    "        val_idx = np.array(list(set(val_idx).difference(set(zero_rows))))\n",
    "        test_idx = np.array(list(set(test_idx).difference(set(zero_rows))))\n",
    "\n",
    "    train_indices = th.tensor(train_idx).to(device).long()\n",
    "    valid_indices = th.tensor(val_idx).to(device).long()\n",
    "    test_indices = th.tensor(test_idx).to(device).long()\n",
    "    ## Minibatch training parameters\n",
    "    params = {'batch_size': batch_size,\n",
    "              'shuffle': True,\n",
    "              'num_workers': 0}\n",
    "    training_set = Dataset(train_idx, labels,feats)\n",
    "    training_generator = th.utils.data.DataLoader(training_set, **params)\n",
    "\n",
    "    validation_set = Dataset(val_idx, labels,feats)\n",
    "    validation_generator = th.utils.data.DataLoader(validation_set, **params)\n",
    "\n",
    "    test_set = Dataset(test_idx, labels,feats)\n",
    "    test_generator = th.utils.data.DataLoader(test_set, **params)\n",
    "\n",
    "\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "    if multilabel is False:\n",
    "        loss_func = th.nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        loss_func = th.nn.BCEWithLogitsLoss()\n",
    "    for epoch in range(n_cepochs):\n",
    "        for local_batch, local_labels in training_generator:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(local_batch)\n",
    "            local_labels = local_labels.to(device)\n",
    "            loss = loss_func(pred, (local_labels))\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if multilabel is False:\n",
    "                pred = pred.argmax(dim=1)\n",
    "            else:\n",
    "                pred = pred\n",
    "            print_debug=False\n",
    "            if print_debug:\n",
    "                train_auc = compute_acc(results=pred, labels=local_labels, multilabel=multilabel)\n",
    "                print('Step loss %.4f, Train Acc %.4f' % (\n",
    "                         loss.item(),\n",
    "                         train_auc.item() if th.is_tensor(train_auc) else train_auc\n",
    "                     ))\n",
    "        if epoch % 5 == 0:\n",
    "            val_acc = evaluate_mlp_model(model,validation_generator,multilabel,device)\n",
    "            test_acc = evaluate_mlp_model(model,test_generator,multilabel,device)\n",
    "\n",
    "            if best_val_acc < val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_test_acc = test_acc\n",
    "            print('Epoch %.1f, Loss %.4f, Val Acc %.4f (Best %.4f), Test Acc %.4f (Best %.4f)' % (\n",
    "                epoch,\n",
    "                loss.item(),\n",
    "                #train_auc.item() if th.is_tensor(train_auc) else train_auc,\n",
    "                val_acc.item()if th.is_tensor(val_acc) else val_acc,\n",
    "                best_val_acc.item()if th.is_tensor(best_val_acc) else best_val_acc,\n",
    "                test_acc.item()if th.is_tensor(test_acc) else test_acc,\n",
    "                best_test_acc.item()if th.is_tensor(best_test_acc) else best_test_acc\n",
    "            ))\n",
    "    return best_val_acc,best_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ogbn-products\n",
      "finish loading ogbn-products\n",
      "finish constructing ogbn-products\n",
      "start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0.0, Loss 2.1725, Val Acc 0.3169 (Best 0.3169), Test Acc 0.2702 (Best 0.2702)\n",
      "Epoch 5.0, Loss 1.4487, Val Acc 0.5242 (Best 0.5242), Test Acc 0.3461 (Best 0.3461)\n",
      "Epoch 10.0, Loss 1.2794, Val Acc 0.5581 (Best 0.5581), Test Acc 0.3627 (Best 0.3627)\n",
      "Epoch 15.0, Loss 1.3839, Val Acc 0.5828 (Best 0.5828), Test Acc 0.3755 (Best 0.3755)\n",
      "Epoch 20.0, Loss 1.3230, Val Acc 0.5927 (Best 0.5927), Test Acc 0.3811 (Best 0.3811)\n",
      "Epoch 25.0, Loss 1.3238, Val Acc 0.5972 (Best 0.5972), Test Acc 0.3834 (Best 0.3834)\n",
      "Epoch 30.0, Loss 1.2007, Val Acc 0.6049 (Best 0.6049), Test Acc 0.3876 (Best 0.3876)\n",
      "Epoch 35.0, Loss 1.2295, Val Acc 0.6136 (Best 0.6136), Test Acc 0.3928 (Best 0.3928)\n",
      "Epoch 40.0, Loss 1.1121, Val Acc 0.6109 (Best 0.6136), Test Acc 0.3910 (Best 0.3928)\n",
      "Epoch 45.0, Loss 1.2097, Val Acc 0.6082 (Best 0.6136), Test Acc 0.3881 (Best 0.3928)\n",
      "Epoch 50.0, Loss 1.1723, Val Acc 0.6151 (Best 0.6151), Test Acc 0.3929 (Best 0.3929)\n",
      "Epoch 55.0, Loss 1.1863, Val Acc 0.6191 (Best 0.6191), Test Acc 0.3958 (Best 0.3958)\n",
      "Epoch 60.0, Loss 1.2115, Val Acc 0.6141 (Best 0.6191), Test Acc 0.3918 (Best 0.3958)\n",
      "Epoch 65.0, Loss 1.2074, Val Acc 0.6112 (Best 0.6191), Test Acc 0.3907 (Best 0.3958)\n",
      "Epoch 70.0, Loss 1.1894, Val Acc 0.6185 (Best 0.6191), Test Acc 0.3948 (Best 0.3958)\n",
      "Epoch 75.0, Loss 1.2281, Val Acc 0.6197 (Best 0.6197), Test Acc 0.3961 (Best 0.3961)\n",
      "Epoch 80.0, Loss 1.2173, Val Acc 0.6111 (Best 0.6197), Test Acc 0.3909 (Best 0.3961)\n",
      "Epoch 85.0, Loss 1.1751, Val Acc 0.6229 (Best 0.6229), Test Acc 0.3987 (Best 0.3987)\n",
      "Epoch 90.0, Loss 1.0578, Val Acc 0.6160 (Best 0.6229), Test Acc 0.3937 (Best 0.3987)\n",
      "Epoch 95.0, Loss 1.1217, Val Acc 0.6224 (Best 0.6229), Test Acc 0.3988 (Best 0.3987)\n"
     ]
    }
   ],
   "source": [
    "g, num_labels = load_ogb('ogbn-products')\n",
    "g.ndata.pop('features')\n",
    "\n",
    "in_feats = 128\n",
    "eigen_vals, eigen_vecs = get_eigen(g, in_feats, 'ogbn-products')\n",
    "nfeat = th.tensor(eigen_vecs * np.sqrt(eigen_vals).reshape((1, len(eigen_vals))),\n",
    "                  dtype=th.float32)\n",
    "labels = g.ndata.pop('labels')\n",
    "\n",
    "hyperparams = {\n",
    "    'batch_size': 1024,\n",
    "    'num_workers': 4,\n",
    "    'num_hidden': 256,\n",
    "    'num_layers': 3,\n",
    "    'dropout': 0.5,\n",
    "    'lr': 0.003,\n",
    "    'num_epochs': 20,\n",
    "    'fanouts': [15,10,5],\n",
    "    'eval_batch_size': 10000\n",
    "}\n",
    "device = th.device('cuda:7')\n",
    "\n",
    "# Define model\n",
    "train_nid = th.nonzero(g.ndata['train_mask'], as_tuple=True)[0]\n",
    "val_nid = th.nonzero(g.ndata['val_mask'], as_tuple=True)[0]\n",
    "test_nid = th.nonzero(~(g.ndata['train_mask'] | g.ndata['val_mask']), as_tuple=True)[0]\n",
    "best_val_acc,best_test_acc = compute_acc_MLP_pytorch(nfeat, labels, train_nid, val_nid, test_nid, num_labels, device,\n",
    "                                                     n_hidden=200, lr_d=1e-3, n_cepochs=100,\n",
    "                                                     l2norm = 0.0001, batch_size=1000, multilabel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
